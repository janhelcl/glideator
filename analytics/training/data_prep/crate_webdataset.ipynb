{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "import webdataset as wds\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import net.preprocessing as preprocessing\n",
    "import gfs.fetch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"../analytics/training/train_data\"\n",
    "VAL_DATA_PATH = \"../analytics/training/val_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "connection_string = \"postgresql://{user}:{password}@{host}:{port}/{db}\".format(\n",
    "    user=os.getenv('DB_USER'),\n",
    "    password=os.getenv('DB_PASSWORD'),\n",
    "    host=os.getenv('DB_HOST'),\n",
    "    port=os.getenv('DB_PORT'),\n",
    "    db=os.getenv('DB_NAME')\n",
    ")\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_webdataset(engine, output_path, weather_features, site_features,\n",
    "                       site_id_col, date_col, chunk_size=1000, total_chunks=None,\n",
    "                       is_validation=False):\n",
    "    \"\"\"\n",
    "    Convert database data to WebDataset format, processing in chunks\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        output_path: path where to save the .tar files\n",
    "        weather_features: list of column names for weather features\n",
    "        site_features: list of column names for site features\n",
    "        site_id_col: column name for site ID\n",
    "        date_col: column name for date\n",
    "        is_validation: whether to process validation or training data\n",
    "        chunk_size: number of samples per shard/chunk\n",
    "        total_chunks: optional limit on number of chunks to process\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Query to get total count (if total_chunks not specified)\n",
    "    if total_chunks is None:\n",
    "        count_query = f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM glideator_fs.features_with_target\n",
    "        WHERE is_validation = {is_validation}\n",
    "        \"\"\"\n",
    "        with engine.connect() as conn:\n",
    "            total_rows = conn.execute(text(count_query)).scalar()\n",
    "        total_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    # Process data in chunks\n",
    "    for chunk_idx in tqdm(range(total_chunks)):\n",
    "        # Create a new tar file for each chunk\n",
    "        shard_name = f\"{output_path}/shard_{chunk_idx:06d}.tar\"\n",
    "        \n",
    "        # Query for this chunk\n",
    "        chunk_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM glideator_fs.features_with_target\n",
    "        WHERE is_validation = {is_validation}\n",
    "        LIMIT {chunk_size}\n",
    "        OFFSET {chunk_idx * chunk_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Process chunk\n",
    "        with engine.connect() as conn:\n",
    "            chunk_df = pd.read_sql(chunk_query, conn)\n",
    "        \n",
    "        # Drop row with nulls\n",
    "        chunk_df = chunk_df.dropna()\n",
    "        # Convert date column to datetime\n",
    "        chunk_df[date_col] = pd.to_datetime(chunk_df[date_col])\n",
    "\n",
    "        thresholds = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "        preprocessing.add_date_features(chunk_df)\n",
    "        preprocessing.add_targets(chunk_df, thresholds=thresholds)\n",
    "        date_cols = ['weekend', 'year', 'day_of_year_sin', 'day_of_year_cos']\n",
    "        target_cols = [f'XC{threshold}' for threshold in thresholds]\n",
    "        \n",
    "        with wds.TarWriter(shard_name) as sink:\n",
    "            for idx, row in chunk_df.iterrows():\n",
    "                # Create sample key\n",
    "                key = f\"{chunk_idx}_{idx:08d}\"\n",
    "                \n",
    "                # Extract and combine features\n",
    "                weather_data_9 = row[weather_features[9]].values.astype(np.float32)\n",
    "                weather_data_12 = row[weather_features[12]].values.astype(np.float32)\n",
    "                weather_data_15 = row[weather_features[15]].values.astype(np.float32)\n",
    "                site_data = row[site_features].values.astype(np.float32)\n",
    "                date_data = row[date_cols].values.astype(np.float32)\n",
    "                site_id = np.int64(row[site_id_col])  # Convert directly to numpy int64\n",
    "                \n",
    "                # Combine all features\n",
    "                features = {\n",
    "                    'weather': {\n",
    "                        '9': torch.tensor(weather_data_9),\n",
    "                        '12': torch.tensor(weather_data_12),\n",
    "                        '15': torch.tensor(weather_data_15)\n",
    "                    },\n",
    "                    'site': torch.tensor(site_data),\n",
    "                    'site_id': torch.tensor(site_id),\n",
    "                    'date': torch.tensor(date_data)\n",
    "                }\n",
    "                \n",
    "                # Extract target\n",
    "                targets = row[target_cols].values.astype(np.float32)\n",
    "                \n",
    "                # Create sample dictionary\n",
    "                sample = {\n",
    "                    \"__key__\": key,\n",
    "                    \"features.pth\": features,\n",
    "                    \"targets.pth\": torch.tensor(targets),\n",
    "                    \"date.pth\": torch.tensor(row['date'].timestamp())\n",
    "                }\n",
    "                \n",
    "                # Write sample to tar file\n",
    "                sink.write(sample)\n",
    "        \n",
    "        del chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = gfs.fetch.get_col_order()\n",
    "\n",
    "references = (\n",
    "    (6, 3),\n",
    "    (12, 0),\n",
    "    (12, 3)\n",
    ")\n",
    "col_names_full = []\n",
    "for run, delta in references:\n",
    "    for col in col_names:\n",
    "        col_names_full.append(f'{col}_{run+delta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_features = {\n",
    "    9: [col for col in col_names_full if col.endswith('9')],\n",
    "    12: [col for col in col_names_full if col.endswith('12')],\n",
    "    15: [col for col in col_names_full if col.endswith('15')],\n",
    "}\n",
    "site_features = ['latitude', 'longitude', 'altitude']\n",
    "site_id_col = 'site_id'\n",
    "date_col = 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 722/722 [2:10:35<00:00, 10.85s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 181/181 [28:51<00:00,  9.57s/it]\n"
     ]
    }
   ],
   "source": [
    "create_webdataset(\n",
    "    engine=engine,\n",
    "    output_path=TRAIN_DATA_PATH,\n",
    "    weather_features=weather_features,  # List of weather feature column names\n",
    "    site_features=site_features,  # List of site feature column names \n",
    "    site_id_col=site_id_col,  # Column name for site ID\n",
    "    date_col=date_col,  # Column name for date\n",
    "    chunk_size=1000,  # Adjust based on your memory constraints\n",
    "    total_chunks=None,  # Process all data\n",
    "    is_validation=False\n",
    ")\n",
    "create_webdataset(\n",
    "    engine=engine,\n",
    "    output_path=VAL_DATA_PATH,\n",
    "    weather_features=weather_features,  # List of weather feature column names\n",
    "    site_features=site_features,  # List of site feature column names\n",
    "    site_id_col=site_id_col,  # Column name for site ID\n",
    "    date_col=date_col,  # Column name for date\n",
    "    chunk_size=1000,  # Adjust based on your memory constraints\n",
    "    total_chunks=None,  # Process all data\n",
    "    is_validation=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-jl",
   "language": "python",
   "name": "env-jl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
